{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy.random\n",
    "import numpy as np\n",
    "import seaborn\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (8, 8)\n",
    "#seaborn.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes #MultinomialNB\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic , silly and tedious . \r\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny . \r\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 rt-polarity.neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \r\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \r\n",
      "effective but too-tepid biopic\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 rt-polarity.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_neg = [l.strip().split() for l in open(\"rt-polarity.neg\", encoding = \"ISO-8859-1\")]\n",
    "data_pos = [l.strip().split() for l in open(\"rt-polarity.pos\", encoding = \"ISO-8859-1\")]\n",
    "\n",
    "dictionary = {}\n",
    "rev_dictionary = {}\n",
    "counts = Counter()\n",
    "for sent in data_pos + data_neg:\n",
    "    for word in sent:\n",
    "        counts[word] += 1\n",
    "        if word not in dictionary:\n",
    "            key = len(dictionary)\n",
    "            dictionary[word] = key\n",
    "            rev_dictionary[key] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npos = len(data_pos)\n",
    "nneg = len(data_neg)\n",
    "n = npos + nneg\n",
    "m = len(dictionary)\n",
    "X = np.zeros((n, m))\n",
    "y = np.zeros((n,))\n",
    "y[:npos] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(data_pos + data_neg):\n",
    "    for word in sent:\n",
    "        j = dictionary[word]\n",
    "        X[i, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode(sent):\n",
    "    x = np.zeros((1, m))\n",
    "    for word in sent.split():\n",
    "        if word in dictionary:\n",
    "            x[0, dictionary[word]] += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sklearn.naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10662, 21401), (10662,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         . \t 30 \t [-2.93901082 -2.95454721]\n",
      "       the \t 0 \t [-3.27148481 -3.27680855]\n",
      "         , \t 23 \t [-3.32813063 -3.23475216]\n",
      "         a \t 16 \t [-3.65123352 -3.55346689]\n",
      "       and \t 11 \t [-3.9166723  -3.62964515]\n",
      "        of \t 34 \t [-3.87851246 -3.69873863]\n",
      "        to \t 4 \t [-4.07290218 -4.21855921]\n",
      "        is \t 2 \t [-4.38305711 -4.36068435]\n",
      "        in \t 122 \t [-4.63505983 -4.60392314]\n",
      "      that \t 12 \t [-4.70068337 -4.66310291]\n",
      "        it \t 81 \t [-4.72242335 -4.8003739 ]\n",
      "        as \t 71 \t [-4.97060298 -5.02442613]\n",
      "       but \t 55 \t [-5.04596184 -5.14121534]\n",
      "      with \t 118 \t [-5.27908493 -5.01876039]\n",
      "      film \t 85 \t [-5.36208185 -5.09139183]\n",
      "      this \t 146 \t [-5.14536919 -5.30155799]\n",
      "       for \t 252 \t [-5.15966443 -5.29111792]\n",
      "       its \t 158 \t [-5.33989394 -5.2532677 ]\n",
      "        an \t 74 \t [-5.4561108  -5.17498478]\n",
      "     movie \t 76 \t [-5.16491379 -5.56797844]\n",
      "      it's \t 129 \t [-5.47387326 -5.4675223 ]\n",
      "        be \t 5 \t [-5.54248218 -5.76409332]\n",
      "        on \t 416 \t [-5.64125323 -5.75225886]\n",
      "       you \t 59 \t [-5.8092708  -5.59979027]\n",
      "       not \t 251 \t [-5.71829902 -5.89626509]\n",
      "        by \t 248 \t [-5.76754696 -5.86154866]\n",
      "     about \t 156 \t [-5.88203016 -5.90719417]\n",
      "      more \t 112 \t [-5.765146   -6.06134485]\n",
      "       one \t 84 \t [-5.90658801 -5.89898618]\n",
      "      like \t 61 \t [-5.71145749 -6.16244096]\n",
      "       has \t 111 \t [-5.99611384 -5.86417678]\n",
      "       are \t 412 \t [-5.90935426 -5.94927611]\n",
      "        at \t 126 \t [-5.9692866  -5.89898618]\n",
      "      from \t 258 \t [-6.01440704 -5.94641488]\n",
      "      than \t 20 \t [-5.85019723 -6.15890114]\n",
      "         \" \t 9 \t [-6.01133484 -6.00222966]\n",
      "       all \t 93 \t [-5.93176416 -6.13446711]\n",
      "        -- \t 95 \t [-6.05843931 -6.03602704]\n",
      "       his \t 310 \t [-6.14574304 -5.96080362]\n",
      "      have \t 64 \t [-5.89833487 -6.2436664 ]\n",
      "        so \t 38 \t [-5.94890744 -6.4572405 ]\n",
      "        if \t 58 \t [-6.18146112 -6.22461821]\n",
      "        or \t 27 \t [-5.99309725 -6.55732396]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for word, c in counts.most_common(1000):\n",
    "    j = dictionary[word]\n",
    "    pi = model.feature_log_prob_[:, j]\n",
    "    if c > 500:\n",
    "        print(\"%10s \\t %d \\t %s\"%(word, j, pi))\n",
    "    features.append((word, pi[1]/pi[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engrossing 0.748111\n",
      " wonderful 0.776461\n",
      "      warm 0.776461\n",
      " inventive 0.776788\n",
      "  powerful 0.778615\n",
      "refreshing 0.784623\n",
      "  captures 0.788538\n",
      "  provides 0.788538\n",
      "  touching 0.791721\n",
      "  portrait 0.799052\n"
     ]
    }
   ],
   "source": [
    "features.sort(key=lambda x: x[1])\n",
    "for w, s in features[:10]:\n",
    "    print(\"%10s %f\"%( w, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stupid 1.269816\n",
      "   routine 1.295109\n",
      "  mediocre 1.301057\n",
      "   generic 1.301057\n",
      "       bad 1.310359\n",
      "      dull 1.315481\n",
      "      flat 1.316063\n",
      "    boring 1.337208\n",
      "     badly 1.369386\n",
      "   unfunny 1.388356\n"
     ]
    }
   ],
   "source": [
    "for w, s in features[-10:]:\n",
    "    print(\"%10s %f\"%( w, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(encode(\"the film was excellent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(encode(\"professor parkes lecture was an engrossing portait of Bayesian ideals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25657334,  0.74342666]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(encode(\"professor parkes lecture was an engrossing portait of Bayesian ideals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99937145e-01,   6.28550516e-05]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(encode(\"professor rush boldly satirized the boring flat dull routine of modern existence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lr = sklearn.linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
